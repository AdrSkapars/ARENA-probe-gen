{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20bb32e",
   "metadata": {},
   "source": [
    "# Do probe training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729248f5",
   "metadata": {},
   "source": [
    "Load the activations and labels from HF, aggregate, and construct datasets to train the probe on (note sklearn doesn't require a validation dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fe1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels\n",
      "loaded activations with shape torch.Size([5000, 336, 3072])\n",
      "calculated attention mask with shape torch.Size([5000, 336])\n",
      "Train: 3500 samples, 1750.0 positives\n",
      "Val:   500 samples, 250.0 positives\n",
      "Test:  1000 samples, 500.0 positives\n"
     ]
    }
   ],
   "source": [
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report\n",
    "from probe_gen.config import ConfigDict\n",
    "\n",
    "# Create train, val, and test datasets\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(\"refusal_llama_3b_prompted_5k\", layer=12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "train_dataset, val_dataset, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, val_size=0.1, test_size=0.2, balance=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2e182",
   "metadata": {},
   "source": [
    "Create a probe and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364c5b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100, Train Loss: 0.3574, Val Loss: 0.3458\n",
      "Epoch 20/100, Train Loss: 0.3214, Val Loss: 0.3363\n",
      "\n",
      "roc_auc: 0.9227200000000001\n"
     ]
    }
   ],
   "source": [
    "# Initialise and fit a probe with the datasets\n",
    "cfg = ConfigDict(\n",
    "    use_bias=True,\n",
    "    normalize=True,\n",
    "    lr=0.0001,\n",
    "    weight_decay=0.0,\n",
    ")\n",
    "probe = probes.TorchLinearProbe(cfg)\n",
    "probe.fit(train_dataset, val_dataset)\n",
    "\n",
    "# Print val results\n",
    "eval_dict, y_pred, y_pred_proba = probe.eval(val_dataset)\n",
    "print('\\nroc_auc:', eval_dict['roc_auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af9fc6",
   "metadata": {},
   "source": [
    "Evaluate the probe on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392df070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.837, 'roc_auc': 0.89418, 'tpr_at_1_fpr': np.float64(0.148)}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.78      0.83       500\n",
      "         1.0       0.80      0.89      0.85       500\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.84      0.84      0.84      1000\n",
      "weighted avg       0.84      0.84      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_dict, y_pred, y_pred_proba = probe.eval(test_dataset)\n",
    "print(eval_dict)\n",
    "print(classification_report(test_dataset['y'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d43e7080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels\n",
      "loaded activations with shape torch.Size([1000, 249, 3072])\n",
      "calculated attention mask with shape torch.Size([1000, 249])\n",
      "Train: 0 samples, 0.0 positives\n",
      "Val:   0 samples, 0.0 positives\n",
      "Test:  1000 samples, 500.0 positives\n",
      "{'accuracy': 0.85, 'roc_auc': 0.9112439999999999, 'tpr_at_1_fpr': np.float64(0.078)}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.89      0.86       500\n",
      "         1.0       0.88      0.81      0.84       500\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.85      0.85      0.85      1000\n",
      "weighted avg       0.85      0.85      0.85      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a seperate test dataset\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(\"refusal_llama_3b_1k\", layer=12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "_, _, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, val_size=0.0, test_size=1.0, balance=True, verbose=True)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_dict, y_pred, y_pred_proba = probe.eval(test_dataset)\n",
    "print(eval_dict)\n",
    "print(classification_report(test_dataset['y'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c2c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels\n",
      "loaded activations with shape torch.Size([1000, 249, 3072])\n",
      "calculated attention mask with shape torch.Size([1000, 249])\n",
      "Train: 0 samples, 0.0 positives\n",
      "Val:   0 samples, 0.0 positives\n",
      "Test:  1000 samples, 500.0 positives\n",
      "{'accuracy': 0.875, 'roc_auc': 0.925292, 'tpr_at_1_fpr': np.float64(0.196)}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87       500\n",
      "         1.0       0.85      0.91      0.88       500\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.88      0.88      0.87      1000\n",
      "weighted avg       0.88      0.88      0.87      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a seperate test dataset\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(\"refusal_llama_3b_prompted_1k\", layer=12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "_, _, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, val_size=0.0, test_size=1.0, balance=True, verbose=True)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_dict, y_pred, y_pred_proba = probe.eval(test_dataset)\n",
    "print(eval_dict)\n",
    "print(classification_report(test_dataset['y'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc94755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels\n",
      "loaded activations with shape torch.Size([1000, 248, 3072])\n",
      "calculated attention mask with shape torch.Size([1000, 248])\n",
      "Train: 0 samples, 0.0 positives\n",
      "Val:   0 samples, 0.0 positives\n",
      "Test:  1000 samples, 500.0 positives\n",
      "{'accuracy': 0.815, 'roc_auc': 0.908852, 'tpr_at_1_fpr': np.float64(0.22)}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.88      0.83       500\n",
      "         1.0       0.87      0.75      0.80       500\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.82      0.81      0.81      1000\n",
      "weighted avg       0.82      0.81      0.81      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a seperate test dataset\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(\"refusal_ministral_8b_1k\", layer=12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "_, _, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, val_size=0.0, test_size=1.0, balance=True, verbose=True)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_dict, y_pred, y_pred_proba = probe.eval(test_dataset)\n",
    "print(eval_dict)\n",
    "print(classification_report(test_dataset['y'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf0729",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################### Evaluating layer 6 #############################\n"
     ]
    }
   ],
   "source": [
    "from probe_gen.standard_experiments.hyperparameter_search import run_full_hyp_search_on_layers\n",
    "\n",
    "# You might not be able to run all layers at once, so can do them in batches like below\n",
    "run_full_hyp_search_on_layers(\n",
    "    'mean_torch', 'refusal_ministral_8b_5k', 'llama_3b', [6,9,12,15,18,21]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb899629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madr-skapars\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc: 0.853504\n",
      "Best params: {'layer': 12, 'lr': 0.0001, 'use_bias': True, 'normalize': True, 'weight_decay': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from probe_gen.standard_experiments.hyperparameter_search import load_best_params_from_search\n",
    "\n",
    "# Can load the best params from the search at any time\n",
    "load_best_params_from_search(\n",
    "    'mean_torch', 'refusal_ministral_8b_5k', 'llama_3b', [6,9,12,15,18,21]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
