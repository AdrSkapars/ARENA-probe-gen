{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fd824",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# If getting 'Could not find project LASR_probe_gen' get key from https://wandb.ai/authorize and paste below\n",
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20bb32e",
   "metadata": {},
   "source": [
    "# Do probe training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729248f5",
   "metadata": {},
   "source": [
    "Load the activations and labels from HF, aggregate, and construct datasets to train the probe on (note sklearn doesn't require a validation dataset). Create a probe and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report\n",
    "from probe_gen.config import ConfigDict\n",
    "\n",
    "# Load the best hyperparameters or set your own\n",
    "probe_type = \"mean\"\n",
    "dataset_name = \"refusal_llama_3b_5k\"\n",
    "cfg = ConfigDict.from_json(probe_type, dataset_name)\n",
    "# cfg = ConfigDict(layer=12, use_bias=True, normalize=True, c=0.001)\n",
    "\n",
    "# Create train, val, and test datasets\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(dataset_name, layer=cfg.layer, verbose=True)\n",
    "if probe_type == \"mean\":\n",
    "    activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "train_dataset, val_dataset, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[3500, 500, 0], verbose=True)\n",
    "\n",
    "# Initialise and fit a probe with the datasets\n",
    "if probe_type == \"mean_torch\":\n",
    "    probe = probes.TorchLinearProbe(cfg)\n",
    "elif probe_type == \"attention_torch\":\n",
    "    probe = probes.TorchAttentionProbe(cfg)\n",
    "probe.fit(train_dataset, val_dataset)\n",
    "\n",
    "# Print val results\n",
    "eval_dict, y_pred, y_pred_proba = probe.eval(val_dataset)\n",
    "print('\\nroc_auc:', eval_dict['roc_auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af9fc6",
   "metadata": {},
   "source": [
    "Evaluate the probe on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval against seperate test datasets\n",
    "for new_dataset_name in [\"refusal_llama_3b_1k\", \"refusal_llama_3b_prompted_1k\", \"refusal_ministral_8b_1k\"]:\n",
    "    print(f\"\\nEvaluating on {new_dataset_name}\")\n",
    "    activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(new_dataset_name, layer=cfg.layer, verbose=True)\n",
    "    if probe_type == \"mean\":\n",
    "        activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "    _, _, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[0, 0, 1000], verbose=True)\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_dict, y_pred, y_pred_proba = probe.eval(test_dataset)\n",
    "    print(eval_dict)\n",
    "    print(classification_report(test_dataset['y'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1cb06",
   "metadata": {},
   "source": [
    "Nice visualisation to see how the probe splits the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ced3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.experiment_plotting import plot_per_class_prediction_distributions\n",
    "\n",
    "plot_per_class_prediction_distributions(test_dataset['y'], y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423e976",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.hyperparameter_search import run_full_hyp_search_on_layers\n",
    "from probe_gen.standard_experiments.hyperparameter_search import load_best_params_from_search\n",
    "\n",
    "probe_type = \"mean\"\n",
    "dataset_name = \"refusal_llama_3b_5k\"\n",
    "model_name = \"llama_3b\"\n",
    "\n",
    "# You might not be able to run all layers at once, so can do them in batches like below\n",
    "layer_list = [6,9,12,15,18,21]\n",
    "run_full_hyp_search_on_layers(\n",
    "    probe_type, dataset_name, model_name, layer_list\n",
    ")\n",
    "# Can load the best params from the search at any time\n",
    "cfg_dict = load_best_params_from_search(\n",
    "    probe_type, dataset_name, model_name, layer_list\n",
    ")\n",
    "# Should add them to the local json if havent already\n",
    "cfg = ConfigDict(cfg_dict)\n",
    "cfg.add_to_json(probe_type, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10af51e",
   "metadata": {},
   "source": [
    "# Testing On-Off Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126890df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify rows of probes and columns of tests\n",
    "test_dataset_names = [\n",
    "    'refusal_llama_3b_1k',\n",
    "    'refusal_llama_3b_prompted_1k',\n",
    "    \"refusal_ministral_8b_1k\"\n",
    "]\n",
    "probes_setup = [\n",
    "    ['mean', 'refusal_llama_3b_5k'],\n",
    "    ['mean', 'refusal_llama_3b_prompted_5k'],\n",
    "    ['mean', 'refusal_ministral_8b_5k']\n",
    "]\n",
    "\n",
    "# Can specify hyperparameters for each row if dont want to load best hyperparameters, can also do a mix of specified and not specified\n",
    "# probes_setup = [\n",
    "#     ['mean', 'refusal_llama_3b_5k', ConfigDict(layer=12, use_bias=False, normalize=True, c=0.001)],\n",
    "#     ['mean', 'refusal_llama_3b_prompted_5k'],\n",
    "#     ['mean', 'refusal_ministral_8b_5k', ConfigDict(layer=12, use_bias=False, normalize=True, c=0.001)]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313364f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.grid_experiments import run_grid_experiment_lean\n",
    "\n",
    "run_grid_experiment_lean(probes_setup, test_dataset_names, \"llama_3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232acb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.grid_experiments import plot_grid_experiment_lean\n",
    "\n",
    "plot_grid_experiment_lean(probes_setup, test_dataset_names, \"llama_3b\", metric=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17116059",
   "metadata": {},
   "source": [
    "Old code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.grid_experiments import run_grid_experiment\n",
    "\n",
    "run_grid_experiment(\n",
    "    ['refusal_llama_3b_5k', 'refusal_llama_3b_prompted_5k', \"refusal_ministral_8b_5k\"], \n",
    "    ['refusal_llama_3b_1k', 'refusal_llama_3b_prompted_1k', \"refusal_ministral_8b_1k\"], \n",
    "    [12, 12, 12], # layer\n",
    "    [False, False, False], # use_bias   (one for each row)\n",
    "    [True, True, True], # normalize\n",
    "    [0.001, 0.001, 0.001], # C\n",
    "    \"llama_3b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e710f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.grid_experiments import plot_grid_experiment\n",
    "\n",
    "plot_grid_experiment(\n",
    "    ['refusal_llama_3b_5k', 'refusal_llama_3b_prompted_5k', \"refusal_ministral_8b_5k\"],  \n",
    "    ['refusal_llama_3b_1k', 'refusal_llama_3b_prompted_1k', \"refusal_ministral_8b_1k\"], \n",
    "    ['On (llama)', 'On (llama prompted)', 'Off (ministral)'],\n",
    "    ['On (llama)', 'On (llama prompted)', 'Off (ministral)'],\n",
    "    [12, 12, 12], # layer\n",
    "    [False, False, False], # use_bias   (one for each row)\n",
    "    [True, True, True], # normalize\n",
    "    [0.001, 0.001, 0.001], # C\n",
    "    \"llama_3b\",\n",
    "    \"roc_auc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af489c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.experiment_plotting import plot_results_table\n",
    "\n",
    "plot_results_table(['refusal_5k_on', 'refusal_5k_off_other_model'], 12, 'mean', True, False, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b9f54",
   "metadata": {},
   "source": [
    "# Layer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ddba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.layer_experiments import run_layer_experiments\n",
    "\n",
    "use_bias = True\n",
    "normalize_inputs = True\n",
    "\n",
    "run_layer_experiments(\n",
    "    \"mean\",\n",
    "    \"lists_llama_3b_story_5k\",\n",
    "    \"llama_3b\",\n",
    "    1.0, # C\n",
    "    [6, 9, 12, 15, 18], \n",
    "    use_bias_options=[True, False], \n",
    "    normalize_options=[True, False], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.standard_experiments.experiment_plotting import plot_layer_experiment\n",
    "\n",
    "plot_layer_experiment([6, 9, 12, 15, 18], 'lists_llama_3b_story_5k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c56f0",
   "metadata": {},
   "source": [
    "# Compare probe directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb495cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report\n",
    "from probe_gen.config import ConfigDict\n",
    "\n",
    "# dataset_list = ['refusal_llama_3b_5k', 'refusal_llama_3b_prompted_5k', 'refusal_ministral_8b_5k']\n",
    "# layer_list = [12, 12, 12]\n",
    "# use_bias_list = [False, False, False]\n",
    "# normalize_list = [True, True, True]\n",
    "# C_list = [0.001, 0.001, 0.001]\n",
    "\n",
    "dataset_list = ['lists_llama_3b_5k', 'lists_llama_3b_prompted_5k', 'lists_qwen_3b_5k']\n",
    "layer_list = [9, 12, 12]\n",
    "use_bias_list = [True, True, False]\n",
    "normalize_list = [False, False, False]\n",
    "C_list = [1, 10, 1]\n",
    "\n",
    "# dataset_list = ['metaphors_llama_3b_5k', 'metaphors_llama_3b_prompted_5k', 'metaphors_qwen_3b_5k']\n",
    "# layer_list = [9, 15, 6]\n",
    "# use_bias_list = [False, False, True]\n",
    "# normalize_list = [False, False, False]\n",
    "# C_list = [1, 1, 1]\n",
    "\n",
    "probes_list = []\n",
    "\n",
    "for i in range(len(dataset_list)):\n",
    "    activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(dataset_list[i], layer_list[i], verbose=True)\n",
    "    activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "    # Create train, val, and test datasets\n",
    "    train_dataset, val_dataset, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, val_size=0.1, test_size=0.2, verbose=True)\n",
    "\n",
    "    probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=use_bias_list[i], C=C_list[i], normalize=normalize_list[i]))\n",
    "    probe.fit(train_dataset, val_dataset)\n",
    "    probes_list.append(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ebc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "biases = []\n",
    "for i in range(len(probes_list)):\n",
    "    weights.append(probes_list[i].classifier.coef_.flatten())\n",
    "    biases.append(probes_list[i].classifier.intercept_.flatten())\n",
    "\n",
    "print(biases)\n",
    "\n",
    "print(cosine_similarity([weights[0]], [weights[1]])[0, 0])\n",
    "print(cosine_similarity([weights[0]], [weights[2]])[0, 0])\n",
    "print(cosine_similarity([weights[1]], [weights[2]])[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report\n",
    "from probe_gen.config import ConfigDict\n",
    "\n",
    "weights_list = []\n",
    "for i in range(5):\n",
    "    activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(\"refusal_llama_3b_5k\", 12, verbose=True)\n",
    "    activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "    # Create train, val, and test datasets\n",
    "    train_dataset, val_dataset, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, val_size=0.1, test_size=0.2, balance=True, verbose=True)\n",
    "\n",
    "    probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=False, C=0.001, normalize=True))\n",
    "    probe.fit(train_dataset, val_dataset)\n",
    "    weights = probe.classifier.coef_\n",
    "    weights_list.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.concatenate(weights_list, axis=0)\n",
    "print(weights.var(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895abd0e",
   "metadata": {},
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake graph\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create the heatmap with seaborn\n",
    "sns.heatmap(\n",
    "    [[0.97,0.924,0.969,0.626], [0.966,0.932,0.96,0.635], [0.968,0.922,0.97,0.6], [0.33,0.345,0.449,0.511]], \n",
    "    xticklabels=['Natural', 'Prompted', 'Other Model', 'Stories'],\n",
    "    yticklabels=['Natural', 'Prompted', 'Other Model', 'Stories'],\n",
    "    annot=True,  # This adds the text annotations\n",
    "    fmt='.2f',   # Format numbers to 3 decimal places\n",
    "    cmap='Greens',  # You can change the colormap\n",
    "    vmin=0.3,\n",
    "    vmax=1,\n",
    "    ax=ax,\n",
    "    annot_kws={\"size\": 20}\n",
    ")\n",
    "\n",
    "# Rotate x-axis labels\n",
    "#plt.xticks(rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Test set', fontsize=14)\n",
    "plt.ylabel('Train set', fontsize=14)\n",
    "ax.set_title(\"Probe (AUROC) Classification of Lists Behaviour in Llama 3B\", fontsize=15)\n",
    "\n",
    "# increase graph size\n",
    "fig.set_size_inches(8, 6)\n",
    "# fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1278804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(\"refusal_5k_on\", 12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "# Create train, val, and test datasets\n",
    "train_dataset, val_dataset, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, val_size=0, test_size=0.2, balance=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c55ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.config import ConfigDict\n",
    "\n",
    "C_values = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "evals_norm_true_bias_true = []\n",
    "evals_norm_true_bias_false = []\n",
    "evals_norm_false_bias_true = []\n",
    "evals_norm_false_bias_false = []\n",
    "\n",
    "for C in C_values:\n",
    "    print(f\"############ {C} #############\")\n",
    "\n",
    "    probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=True, C=C, normalize=True))\n",
    "    probe.fit(train_dataset, val_dataset)\n",
    "    eval_dict, _, _ = probe.eval(test_dataset)\n",
    "    evals_norm_true_bias_true.append(eval_dict)\n",
    "\n",
    "    probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=False, C=C, normalize=True))\n",
    "    probe.fit(train_dataset, val_dataset)\n",
    "    eval_dict, _, _ = probe.eval(test_dataset)\n",
    "    evals_norm_true_bias_false.append(eval_dict)\n",
    "\n",
    "    probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=True, C=C, normalize=False))\n",
    "    probe.fit(train_dataset, val_dataset)\n",
    "    eval_dict, _, _ = probe.eval(test_dataset)\n",
    "    evals_norm_false_bias_true.append(eval_dict)\n",
    "\n",
    "    probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=False, C=C, normalize=False))\n",
    "    probe.fit(train_dataset, val_dataset)\n",
    "    eval_dict, _, _ = probe.eval(test_dataset)\n",
    "    evals_norm_false_bias_false.append(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3245d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract accuracy and ROC-AUC values for each configuration\n",
    "def extract_metrics(results_list):\n",
    "    accuracy = [result['accuracy'] for result in results_list]\n",
    "    roc_auc = [result['roc_auc'] for result in results_list]\n",
    "    return accuracy, roc_auc\n",
    "\n",
    "# Extract metrics for all configurations\n",
    "acc1, roc1 = extract_metrics(evals_norm_true_bias_true)\n",
    "acc2, roc2 = extract_metrics(evals_norm_true_bias_false)\n",
    "acc3, roc3 = extract_metrics(evals_norm_false_bias_true)\n",
    "acc4, roc4 = extract_metrics(evals_norm_false_bias_false)\n",
    "\n",
    "# Create the plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Define labels for each configuration (modify these based on your actual hyperparameter combinations)\n",
    "config_labels = [\n",
    "    'normalize=True, use_bias=True',  # Replace with actual hyperparameter values, e.g., 'penalty=l2, solver=lbfgs'\n",
    "    'normalize=True, use_bias=False',  # e.g., 'penalty=l2, solver=liblinear'\n",
    "    'normalize=False, use_bias=True',  # e.g., 'penalty=l1, solver=liblinear'\n",
    "    'normalize=False, use_bias=False'   # e.g., 'penalty=elasticnet, solver=saga'\n",
    "]\n",
    "\n",
    "# Plot accuracy (left subplot)\n",
    "ax1.semilogx(C_values, acc1, marker='o', label=config_labels[0])\n",
    "ax1.semilogx(C_values, acc2, marker='s', label=config_labels[1])\n",
    "ax1.semilogx(C_values, acc3, marker='^', label=config_labels[2])\n",
    "ax1.semilogx(C_values, acc4, marker='d', label=config_labels[3])\n",
    "\n",
    "ax1.set_xlabel('C (Inverse Regularization Strength)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy vs C')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot ROC-AUC (right subplot)\n",
    "ax2.semilogx(C_values, roc1, marker='o', label=config_labels[0])\n",
    "ax2.semilogx(C_values, roc2, marker='s', label=config_labels[1])\n",
    "ax2.semilogx(C_values, roc3, marker='^', label=config_labels[2])\n",
    "ax2.semilogx(C_values, roc4, marker='d', label=config_labels[3])\n",
    "\n",
    "ax2.set_xlabel('C (Inverse Regularization Strength)')\n",
    "ax2.set_ylabel('ROC-AUC')\n",
    "ax2.set_title('ROC-AUC vs C')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Find and print the best configuration for each metric\n",
    "def find_best_config(metric_lists, config_labels, C_values):\n",
    "    best_scores = []\n",
    "    for i, metrics in enumerate(metric_lists):\n",
    "        best_idx = np.argmax(metrics)\n",
    "        best_scores.append({\n",
    "            'config': config_labels[i],\n",
    "            'best_C': C_values[best_idx],\n",
    "            'best_score': metrics[best_idx]\n",
    "        })\n",
    "    return best_scores\n",
    "\n",
    "# Find best configurations\n",
    "accuracy_lists = [acc1, acc2, acc3, acc4]\n",
    "roc_lists = [roc1, roc2, roc3, roc4]\n",
    "\n",
    "print(\"Best Accuracy Results:\")\n",
    "best_acc = find_best_config(accuracy_lists, config_labels, C_values)\n",
    "for result in best_acc:\n",
    "    print(f\"{result['config']}: C={result['best_C']}, Accuracy={result['best_score']:.4f}\")\n",
    "\n",
    "print(\"\\nBest ROC-AUC Results:\")\n",
    "best_roc = find_best_config(roc_lists, config_labels, C_values)\n",
    "for result in best_roc:\n",
    "    print(f\"{result['config']}: C={result['best_C']}, ROC-AUC={result['best_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
