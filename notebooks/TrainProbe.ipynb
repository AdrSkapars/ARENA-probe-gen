{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b38c53",
   "metadata": {},
   "source": [
    "## Train probe\n",
    "- Goal: To train MVP probes on activations\n",
    "    - Maybe make those probes be fancy (mean, attention, softmax)\n",
    "    - Maybe make those activations be real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20bb32e",
   "metadata": {},
   "source": [
    "### Do probe training (HF dataset activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729248f5",
   "metadata": {},
   "source": [
    "Load the activations and labels from HF, aggregate, and construct datasets to train the probe on (note sklearn doesn't require a validation dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the activations and labels from hf\n",
    "activations_tensor, labels_tensor = probes.load_hf_activations_and_labels(\"NLie2/anthropic-refusal-activations\", \"refusal_meta-llama_Llama-3.2-3B-Instruct__on_policy\", 20)\n",
    "# Aggregate the activations\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor)\n",
    "# Create train, val, and test datasets\n",
    "train_dataset, val_dataset, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, val_size=0, test_size=0.2, balance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e80507",
   "metadata": {},
   "source": [
    "Create a probe and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be293935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise and fit a probe with the datasets\n",
    "probe = probes.SklearnLogisticProbe(use_bias=False)\n",
    "probe.fit(train_dataset, val_dataset, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af9fc6",
   "metadata": {},
   "source": [
    "Evaluate the probe on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26fc8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = probe.predict(test_dataset['X'])\n",
    "y_pred_proba = probe.predict_proba(test_dataset['X'])  # probabilities for class 1\n",
    "\n",
    "# Evaluate the model\n",
    "eval_dict = probe.eval(test_dataset)\n",
    "print(eval_dict)\n",
    "print(type(classification_report(test_dataset['y'], y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1cb06",
   "metadata": {},
   "source": [
    "Nice visualisation to see how the probe splits the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de150007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.hist(y_pred_proba[test_dataset['y'].numpy() == 0], alpha=0.5, label='Complied', bins=20)\n",
    "plt.hist(y_pred_proba[test_dataset['y'].numpy() == 1], alpha=0.5, label='Rejected', bins=20)\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.title('Prediction Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d303b",
   "metadata": {},
   "source": [
    "# Torch training (not yet integrated / needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb13be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now actually train the probe on training data and evaluate on test data\n",
    "import matplotlib.pyplot as plt\n",
    "from probe_gen.probe_train import train_probe, evaluate_probe\n",
    "\n",
    "print(\"Training linear probe...\")\n",
    "print(f\"Train data: {train_activations.shape}, Val data: {val_activations.shape}\")\n",
    "\n",
    "# Train probe\n",
    "model, train_losses, val_losses = train_probe(\n",
    "    train_activations=train_activations,\n",
    "    train_labels=train_labels,\n",
    "    val_activations=val_activations,\n",
    "    val_labels=val_labels,\n",
    "    probe_type=\"mean\",\n",
    "    learning_rate=5e-3,\n",
    "    batch_size=16,\n",
    "    num_epochs=100,\n",
    "    early_stop_patience=20,\n",
    "    silent=True\n",
    ")\n",
    "\n",
    "# Evaluate probe\n",
    "print(\"\\nEvaluating probe...\")\n",
    "results = evaluate_probe(model, test_activations, test_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"Test AUC-ROC: {results['auc_roc']:.4f}\")\n",
    "print(f\"TPR at 1% FPR: {results['tpr_at_1_fpr']:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Curves')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(results['probabilities'][test_labels.numpy() == 0], alpha=0.5, label='Complied', bins=20)\n",
    "plt.hist(results['probabilities'][test_labels.numpy() == 1], alpha=0.5, label='Rejected', bins=20)\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.title('Prediction Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Show some values\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "for i in range(10):\n",
    "    logits = model(train_activations[i:i+1,:,:])\n",
    "    print(train_labels[i], logits[0])\n",
    "    print(torch.sigmoid(logits[0]))\n",
    "    print(torch.sigmoid(logits[0]) > 0.5)\n",
    "    print(criterion(logits[0], train_labels[i].float()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd74a7",
   "metadata": {},
   "source": [
    "### Do probe experiments (HF dataset activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94002ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "from probe_gen.probe_train import train_probe, evaluate_probe\n",
    "\n",
    "for layer_num in range(5):\n",
    "    print(f\"\\n\\nLayer {layer_num}\")\n",
    "    # Load the labels\n",
    "    labels_list = []\n",
    "    with open(\"../data/refusal/on_policy_raw.jsonl\", 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if data[\"scale_labels\"] <= 5:\n",
    "                labels_list.append(1.0)\n",
    "            else:\n",
    "                labels_list.append(0.0)\n",
    "    labels_tensor = torch.tensor(labels_list)\n",
    "    print(\"loaded labels\")\n",
    "\n",
    "    # Load activations\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=\"NLie2/anthropic-refusal-activations\",\n",
    "        filename=\"dataframe_with_activations.pkl\",\n",
    "        repo_type=\"dataset\")\n",
    "    df = pd.read_pickle(file_path)\n",
    "    print(f\"loaded activations, with shape: {df.shape}\")\n",
    "\n",
    "    # Extract all activations\n",
    "    all_activations = []\n",
    "    for i in range(len(df)):\n",
    "        all_activations.append(df.loc[i][\"activations\"][0])\n",
    "    activations_tensor = torch.cat(all_activations, dim=0).unsqueeze(1).to(torch.float32)\n",
    "\n",
    "    # Get indices for each label\n",
    "    label_0_indices = (labels_tensor == 0.0).nonzero(as_tuple=True)[0]\n",
    "    label_1_indices = (labels_tensor == 1.0).nonzero(as_tuple=True)[0]\n",
    "    min_class_count = min(len(label_0_indices), len(label_1_indices))\n",
    "\n",
    "    # Subsample both classes to same size\n",
    "    label_0_indices = label_0_indices[:min_class_count]\n",
    "    label_1_indices = label_1_indices[:min_class_count]\n",
    "\n",
    "    # Shuffle indices\n",
    "    label_0_indices = label_0_indices[torch.randperm(len(label_0_indices))]\n",
    "    label_1_indices = label_1_indices[torch.randperm(len(label_1_indices))]\n",
    "\n",
    "    # Compute split sizes\n",
    "    n_total = min_class_count\n",
    "    n_train = int(0.8 * n_total)\n",
    "    n_val = int(0.1 * n_total)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    # Split label 0s\n",
    "    train_0 = label_0_indices[:n_train]\n",
    "    val_0 = label_0_indices[n_train:n_train + n_val]\n",
    "    test_0 = label_0_indices[n_train + n_val:]\n",
    "\n",
    "    # Split label 1s\n",
    "    train_1 = label_1_indices[:n_train]\n",
    "    val_1 = label_1_indices[n_train:n_train + n_val]\n",
    "    test_1 = label_1_indices[n_train + n_val:]\n",
    "\n",
    "    # Concatenate splits and shuffle within each\n",
    "    def get_split(indices_0, indices_1):\n",
    "        indices = torch.cat([indices_0, indices_1])\n",
    "        indices = indices[torch.randperm(len(indices))]\n",
    "        return activations_tensor[indices], labels_tensor[indices]\n",
    "\n",
    "    train_activations, train_labels = get_split(train_0, train_1)\n",
    "    val_activations, val_labels = get_split(val_0, val_1)\n",
    "    test_activations, test_labels = get_split(test_0, test_1)\n",
    "\n",
    "    # Confirm balance\n",
    "    print(f\"Train: {train_labels.shape[0]} samples, {train_labels.sum()} positives\")\n",
    "    print(f\"Val:   {val_labels.shape[0]} samples, {val_labels.sum()} positives\")\n",
    "    print(f\"Test:  {test_labels.shape[0]} samples, {test_labels.sum()} positives\")\n",
    "\n",
    "    # Now actually train the probe on training data and evaluate on test data\n",
    "\n",
    "    print(\"Training linear probe...\")\n",
    "    print(f\"Train data: {train_activations.shape}, Val data: {val_activations.shape}\")\n",
    "\n",
    "    # Train probe\n",
    "    model, train_losses, val_losses = train_probe(\n",
    "        train_activations=train_activations,\n",
    "        train_labels=train_labels,\n",
    "        val_activations=val_activations,\n",
    "        val_labels=val_labels,\n",
    "        probe_type=\"mean\",\n",
    "        learning_rate=5e-3,\n",
    "        batch_size=16,\n",
    "        num_epochs=100,\n",
    "        early_stop_patience=20,\n",
    "        silent=True\n",
    "    )\n",
    "\n",
    "    # Evaluate probe\n",
    "    print(\"\\nEvaluating probe...\")\n",
    "    results = evaluate_probe(model, test_activations, test_labels)\n",
    "\n",
    "    print(\"Test Accuracy, Test AUC-ROC, TPR at 1% FPR\")\n",
    "    print(f\"{results['accuracy']:.4f}\\t{results['auc_roc']:.4f}\\t{results['tpr_at_1_fpr']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
