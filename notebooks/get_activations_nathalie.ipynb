{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7135a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer \n",
    "from datasets import load_dataset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbeec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to HF (paste token) \n",
    "login(token= \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5d8dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f19bccaaca47dba0b2d3b942ea0ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pad token for meta-llama/Llama-3.2-3B-Instruct: <|finetune_right_pad_id|>\n",
      "Pad token set to: <|finetune_right_pad_id|>\n"
     ]
    }
   ],
   "source": [
    "def get_pad_token(model_id, tokenizer):\n",
    "    if tokenizer.pad_token is not None:\n",
    "        return tokenizer.pad_token\n",
    "    model_to_pad_token = {\n",
    "        \"meta-llama/Llama-3.2-3B-Instruct\": \"<|finetune_right_pad_id|>\",\n",
    "        \"google/gemma-7b-it\": \"<pad>\",\n",
    "        \"mistralai/Ministral-8B-Instruct-2410\": \"<pad>\",\n",
    "        \"meta-llama/Llama-3.1-8B-Instruct\": \"<|finetune_right_pad_id|>\"\n",
    "    }\n",
    "    pad_token = model_to_pad_token[model_id]\n",
    "    print(f\"Using pad token for {model_id}: {pad_token}\")\n",
    "    return pad_token\n",
    "\n",
    "def get_model(model_name: str):\n",
    "    # Load the model with float16 precision\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,  # Added float16 precision\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set the pad token using the same logic as control experiments\n",
    "    tokenizer.pad_token = get_pad_token(model_name, tokenizer)\n",
    "    print(f\"Pad token set to: {tokenizer.pad_token}\")\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = get_model(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfdd7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_layers_to_enumerate(model):\n",
    "    model_name = model.config._name_or_path\n",
    "    if 'gpt' in model_name:\n",
    "        return model.transformer.h\n",
    "    elif 'pythia' in model_name:\n",
    "        return model.gpt_neox.layers\n",
    "    elif 'bert' in model_name:\n",
    "        return model.encoder.layer\n",
    "    elif 'mistral' in model_name:\n",
    "        return model.model.layers\n",
    "    elif 'gemma' in model_name:\n",
    "        return model.model.layers\n",
    "    elif \"llama\" in model_name:\n",
    "        return model.model.layers\n",
    "    elif \"qwen\" in model_name.lower():\n",
    "        return model.model.layers\n",
    "    else:\n",
    "      raise ValueError(f\"Unsupported model: {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2593d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_activations(model, tokenizer, prompt):\n",
    "\n",
    "    \"\"\"Simplest version - just get activations from all layers\"\"\"    \n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs.input_ids.shape[1]  # Store original length\n",
    "\n",
    "    # Storage for activations\n",
    "    activations = {}\n",
    "    \n",
    "\n",
    "    # Simple hook function\n",
    "    def get_activation(name):\n",
    "\n",
    "        def hook(model, input, output):\n",
    "\n",
    "            activations[name] = output.detach().cpu()\n",
    "\n",
    "        return hook\n",
    "    \n",
    "\n",
    "    # Get layers and register hooks\n",
    "    layers_to_enum = get_res_layers_to_enumerate(model)\n",
    "\n",
    "    hooks = [] \n",
    "\n",
    "    for i, layer in enumerate(layers_to_enum):\n",
    "\n",
    "        hook_handle = layer.register_forward_hook(get_activation(i))\n",
    "\n",
    "        hooks.append(hook_handle)\n",
    "\n",
    "    # Run forward pass\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 200, do_sample = True, pad_token_id = tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode the full generated sequence\n",
    "    new_tokens = outputs[0][input_length:]  # Skip the input tokens\n",
    "    decoded_outputs = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    print(decoded_outputs)\n",
    "    \n",
    "\n",
    "    # Clean up hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    \n",
    "    return activations, decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b566a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How about you? How's your day going?\n",
      "{0: tensor([[[-0.0048,  0.0000,  0.0235,  ..., -0.0091,  0.0082, -0.0044]]],\n",
      "       dtype=torch.float16), 1: tensor([[[-0.0094,  0.0144,  0.0406,  ..., -0.0247,  0.0312, -0.0238]]],\n",
      "       dtype=torch.float16), 2: tensor([[[-0.0015,  0.0246,  0.0760,  ..., -0.0502, -0.0155, -0.0393]]],\n",
      "       dtype=torch.float16), 3: tensor([[[ 0.0242,  0.0248,  0.0917,  ..., -0.1015, -0.0359, -0.0090]]],\n",
      "       dtype=torch.float16), 4: tensor([[[ 0.0635, -0.0667,  0.1449,  ..., -0.1262, -0.1207,  0.0222]]],\n",
      "       dtype=torch.float16), 5: tensor([[[ 0.0020,  0.0266,  0.2401,  ..., -0.0441, -0.0747, -0.0853]]],\n",
      "       dtype=torch.float16), 6: tensor([[[-0.1353,  0.0723,  0.2620,  ..., -0.0273, -0.1289, -0.0744]]],\n",
      "       dtype=torch.float16), 7: tensor([[[-0.0563, -0.1140,  0.2749,  ...,  0.0500,  0.0791, -0.0269]]],\n",
      "       dtype=torch.float16), 8: tensor([[[-0.1212, -0.1282,  0.2898,  ..., -0.0858, -0.0004,  0.0195]]],\n",
      "       dtype=torch.float16), 9: tensor([[[-0.1862, -0.0964,  0.2761,  ..., -0.0933, -0.0033,  0.0132]]],\n",
      "       dtype=torch.float16), 10: tensor([[[-0.1691,  0.0090,  0.3052,  ..., -0.2998, -0.0771, -0.0020]]],\n",
      "       dtype=torch.float16), 11: tensor([[[-0.0699,  0.0307,  0.2947,  ..., -0.2163, -0.0854, -0.0802]]],\n",
      "       dtype=torch.float16), 12: tensor([[[ 0.0163,  0.0852,  0.3074,  ..., -0.3420, -0.2355, -0.0978]]],\n",
      "       dtype=torch.float16), 13: tensor([[[-0.1270, -0.1620,  0.2634,  ..., -0.3596, -0.3604, -0.0363]]],\n",
      "       dtype=torch.float16), 14: tensor([[[-0.1058, -0.0638,  0.2173,  ..., -0.2795, -0.3667, -0.0404]]],\n",
      "       dtype=torch.float16), 15: tensor([[[ 0.0233, -0.1816,  0.3025,  ..., -0.0795, -0.4272, -0.1631]]],\n",
      "       dtype=torch.float16), 16: tensor([[[-0.0372, -0.2747,  0.3015,  ...,  0.0171, -0.2976, -0.2120]]],\n",
      "       dtype=torch.float16), 17: tensor([[[-0.2136, -0.1643,  0.3613,  ..., -0.0268, -0.2573, -0.1411]]],\n",
      "       dtype=torch.float16), 18: tensor([[[-0.0131, -0.2505,  0.4116,  ...,  0.0171, -0.4790, -0.0795]]],\n",
      "       dtype=torch.float16), 19: tensor([[[-0.0946, -0.2969,  0.3169,  ...,  0.1396, -0.2622,  0.0598]]],\n",
      "       dtype=torch.float16), 20: tensor([[[ 0.0529, -0.1934,  0.3533,  ...,  0.1118, -0.1643,  0.1747]]],\n",
      "       dtype=torch.float16), 21: tensor([[[-0.0354, -0.1133,  0.4233,  ..., -0.0261, -0.2205,  0.0829]]],\n",
      "       dtype=torch.float16), 22: tensor([[[-0.3206, -0.1306,  0.2340,  ...,  0.1060, -0.1703, -0.0670]]],\n",
      "       dtype=torch.float16), 23: tensor([[[-0.5312, -0.2467,  0.2930,  ...,  0.1699, -0.4209,  0.1521]]],\n",
      "       dtype=torch.float16), 24: tensor([[[-0.9097, -0.0089,  0.4719,  ...,  0.3132, -0.2104,  0.6748]]],\n",
      "       dtype=torch.float16), 25: tensor([[[-0.7158,  0.1016,  0.3403,  ...,  0.3120, -0.2156,  0.7144]]],\n",
      "       dtype=torch.float16), 26: tensor([[[-0.8301,  0.0458,  0.6572,  ...,  0.0269, -0.6504,  0.6348]]],\n",
      "       dtype=torch.float16), 27: tensor([[[-0.9419, -0.2937, -0.1436,  ...,  0.2366, -0.9033,  0.9512]]],\n",
      "       dtype=torch.float16)}\n",
      "I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How about you? How's your day going?\n"
     ]
    }
   ],
   "source": [
    "# format prompt \n",
    "def format_conversation(tokenizer, messages):\n",
    "    \"\"\"Format messages according to the model's chat template\"\"\"\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# Single question - this is the correct way\n",
    "messages = [{\"role\": \"user\", \"content\": \"How do you do?\"}]\n",
    "prompt = format_conversation(tokenizer, messages)\n",
    "\n",
    "a, o = get_res_activations(model, tokenizer, prompt)\n",
    "\n",
    "print(a)\n",
    "\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cae773ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor function to get activations for multiple prompts at once\n",
    "def get_batch_res_activations(model, tokenizer, prompts):\n",
    "    \"\"\"Efficient batch version - get activations from all layers for multiple prompts\"\"\"\n",
    "    # Handle single prompt case\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "    \n",
    "    # Tokenize all inputs with padding\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,  # Pad to same length\n",
    "        truncation=True,  # Handle very long sequences\n",
    "        max_length=512  # Adjust as needed\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Storage for activations - now batch_size x seq_len x hidden_dim\n",
    "    activations = {}\n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            # Store the full batch of activations\n",
    "            activations[name] = output.detach().cpu()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks once for all prompts\n",
    "    layers_to_enum = get_res_layers_to_enumerate(model)\n",
    "    hooks = []\n",
    "    for i, layer in enumerate(layers_to_enum):\n",
    "        hook_handle = layer.register_forward_hook(get_activation(i))\n",
    "        hooks.append(hook_handle)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generate for entire batch\n",
    "        outputs = model.generate(\n",
    "            **inputs,  # This already contains input_ids and attention_mask\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    # Decode raw outputs (no cleaning)\n",
    "    raw_outputs = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        full_decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        raw_outputs.append(full_decoded)\n",
    "        # print(f\"Response {i+1}: {full_decoded}\")\n",
    "    \n",
    "    # Clean up hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    return activations, raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc6bbf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "def format_prompts_from_strings(tokenizer, prompt_strings):\n",
    "    \"\"\"\n",
    "    Takes a list of string prompts and formats them for the model\n",
    "    Input: [\"how do you do\", \"what is your favorite condiment\", ...]\n",
    "    Output: List of properly formatted prompts\n",
    "    \"\"\"\n",
    "    formatted_prompts = []\n",
    "    for prompt_string in prompt_strings:\n",
    "        # Convert string to message format\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_string}]\n",
    "        # Apply chat template\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        formatted_prompts.append(formatted_prompt)\n",
    "    return formatted_prompts\n",
    "\n",
    "\n",
    "# Usage - exactly what you want:\n",
    "prompts = [\"how do you do\", \"what is your favorite condiment\", \"tell me about cats\", \"explain gravity\"]\n",
    "formatted_prompts = format_prompts_from_strings(tokenizer, prompts)\n",
    "\n",
    "# Then use with your batch function:\n",
    "activations, raw_outputs = get_batch_res_activations(model, tokenizer, formatted_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcf7b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "def process_dataframe_dataset(model, tokenizer, df, prompt_column, batch_size=16, save_every=100):\n",
    "    \"\"\"\n",
    "    Process a pandas DataFrame and add activations as a new column\n",
    "    \n",
    "    Args:\n",
    "        model: The model to get activations from\n",
    "        tokenizer: Tokenizer for the model\n",
    "        df: pandas DataFrame containing the prompts\n",
    "        prompt_column: Name of the column containing the prompt strings\n",
    "        batch_size: Number of examples to process at once\n",
    "        save_every: Print progress every N batches\n",
    "    \n",
    "    Returns:\n",
    "        df: DataFrame with new 'activations' and 'outputs' columns added\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Extract prompts from the specified column\n",
    "    dataset = df_copy[prompt_column].tolist()\n",
    "    \n",
    "    # Format all prompts upfront\n",
    "    print(\"Formatting prompts...\")\n",
    "    formatted_prompts = format_prompts_from_strings(tokenizer, dataset)\n",
    "    \n",
    "    # Storage for all results\n",
    "    all_activations = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    num_batches = (len(formatted_prompts) + batch_size - 1) // batch_size\n",
    "    print(f\"Processing {len(formatted_prompts)} examples in {num_batches} batches of size {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(formatted_prompts))\n",
    "        \n",
    "        batch_prompts = formatted_prompts[start_idx:end_idx]\n",
    "        \n",
    "        try:\n",
    "            # Get activations for this batch\n",
    "            batch_activations, batch_outputs = get_batch_res_activations(\n",
    "                model, tokenizer, batch_prompts\n",
    "            )\n",
    "            \n",
    "            # Convert batch activations to per-example format\n",
    "            batch_size_actual = len(batch_prompts)\n",
    "            for example_idx in range(batch_size_actual):\n",
    "                # Extract activations for this specific example\n",
    "                example_activations = {}\n",
    "                for layer_idx, layer_acts in batch_activations.items():\n",
    "                    # layer_acts shape: [batch_size, seq_len, hidden_dim]\n",
    "                    example_activations[layer_idx] = layer_acts[example_idx]  # [seq_len, hidden_dim]\n",
    "                \n",
    "                all_activations.append(example_activations)\n",
    "                all_outputs.append(batch_outputs[example_idx])\n",
    "            \n",
    "            # Clean up GPU memory\n",
    "            del batch_activations\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            gc.collect()\n",
    "            \n",
    "            # Progress update\n",
    "            if (batch_idx + 1) % save_every == 0:\n",
    "                print(f\"Processed {len(all_outputs)} examples so far...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "            # Add empty results for failed examples to maintain alignment\n",
    "            batch_size_actual = len(batch_prompts)\n",
    "            for _ in range(batch_size_actual):\n",
    "                all_activations.append({})  # Empty dict for failed examples\n",
    "                all_outputs.append(\"\")       # Empty string for failed outputs\n",
    "            continue\n",
    "    \n",
    "    # Add results as new columns to the DataFrame\n",
    "    df_copy['activations'] = all_activations\n",
    "    df_copy['model_outputs'] = all_outputs\n",
    "    \n",
    "    # Save the enhanced DataFrame\n",
    "    print(\"Saving enhanced DataFrame...\")\n",
    "    try:\n",
    "        # Save as pickle to preserve the tensor data in activations\n",
    "        df_copy.to_pickle('dataframe_with_activations.pkl')\n",
    "        print(\"Enhanced DataFrame saved to 'dataframe_with_activations.pkl'\")\n",
    "        \n",
    "        # Also save as parquet (more efficient for large datasets)\n",
    "        # Note: parquet might not handle complex nested structures well\n",
    "        # df_copy.to_parquet('enhanced_dataframe_with_activations.parquet')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save enhanced DataFrame: {e}\")\n",
    "        print(\"DataFrame is still returned in memory\")\n",
    "    \n",
    "    print(f\"Completed processing {len(all_outputs)} examples\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def format_prompts_from_strings(tokenizer, prompt_strings):\n",
    "    \"\"\"\n",
    "    Takes a list of string prompts and formats them for the model\n",
    "    \"\"\"\n",
    "    formatted_prompts = []\n",
    "    for prompt_string in prompt_strings:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_string}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        formatted_prompts.append(formatted_prompt)\n",
    "    return formatted_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2183f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting prompts...\n",
      "Processing 10 examples in 2 batches of size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                   | 0/2 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:  50%|█████████████████████████████████████████████████████████████████████▌                                                                     | 1/2 [00:06<00:06,  6.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving enhanced DataFrame...\n",
      "Enhanced DataFrame saved to 'dataframe_with_activations.pkl'\n",
      "Completed processing 10 examples\n",
      "New DataFrame structure:\n",
      "['id', 'prompt', 'category', 'other_data', 'activations', 'model_outputs']\n",
      "Row 0 activations keys: dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])\n",
      "Layer 0 activation shape: torch.Size([1, 3072])\n",
      "Row 0 model output: system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 06 Aug 2025\n",
      "\n",
      "user\n",
      "\n",
      "how do you doassistant\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example DataFrame - create 10 examples by repeating the pattern\n",
    "    base_prompts = [\n",
    "        \"how do you do\",\n",
    "        \"what is your favorite condiment\", \n",
    "        \"tell me about cats\",\n",
    "        \"explain gravity\"\n",
    "    ]\n",
    "    \n",
    "    # Create 10 prompts by cycling through the base prompts\n",
    "    prompts_10 = []\n",
    "    for i in range(10):\n",
    "        prompts_10.append(base_prompts[i % len(base_prompts)])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'id': range(10),\n",
    "        'prompt': prompts_10,\n",
    "        'category': ['question'] * 10,\n",
    "        'other_data': ['some_value'] * 10\n",
    "    })\n",
    "    \n",
    "    # Process the DataFrame\n",
    "    enhanced_df = process_dataframe_dataset(\n",
    "        model, tokenizer, df, \n",
    "        prompt_column='prompt',  # Name of column with prompts\n",
    "        batch_size=8,\n",
    "        save_every=50\n",
    "    )\n",
    "    \n",
    "    # Now your DataFrame has new columns:\n",
    "    print(\"New DataFrame structure:\")\n",
    "    print(enhanced_df.columns.tolist())\n",
    "    \n",
    "    # Access activations for a specific row\n",
    "    row_0_activations = enhanced_df.loc[0, 'activations']\n",
    "    print(f\"Row 0 activations keys: {row_0_activations.keys()}\")\n",
    "    print(f\"Layer 0 activation shape: {row_0_activations[0].shape}\")\n",
    "    \n",
    "    # Access model output for a specific row\n",
    "    row_0_output = enhanced_df.loc[0, 'model_outputs']\n",
    "    print(f\"Row 0 model output: {row_0_output[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b96cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved DataFrame later:\n",
    "def load_enhanced_dataframe(filepath='enhanced_dataframe_with_activations.pkl'):\n",
    "    \"\"\"Load the enhanced DataFrame with activations\"\"\"\n",
    "    return pd.read_pickle(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33aca0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'prompt', 'category', 'other_data', 'activations',\n",
      "       'model_outputs'],\n",
      "      dtype='object')\n",
      "   id                           prompt  category  other_data  \\\n",
      "0   0                    how do you do  question  some_value   \n",
      "1   1  what is your favorite condiment  question  some_value   \n",
      "2   2               tell me about cats  question  some_value   \n",
      "3   3                  explain gravity  question  some_value   \n",
      "4   4                    how do you do  question  some_value   \n",
      "\n",
      "                                         activations  \\\n",
      "0  {0: [[tensor(0.0070, dtype=torch.float16), ten...   \n",
      "1  {0: [[tensor(0.0064, dtype=torch.float16), ten...   \n",
      "2  {0: [[tensor(0.0184, dtype=torch.float16), ten...   \n",
      "3  {0: [[tensor(-0.0277, dtype=torch.float16), te...   \n",
      "4  {0: [[tensor(0.0067, dtype=torch.float16), ten...   \n",
      "\n",
      "                                       model_outputs  \n",
      "0  system\\n\\nCutting Knowledge Date: December 202...  \n",
      "1  system\\n\\nCutting Knowledge Date: December 202...  \n",
      "2  system\\n\\nCutting Knowledge Date: December 202...  \n",
      "3  system\\n\\nCutting Knowledge Date: December 202...  \n",
      "4  system\\n\\nCutting Knowledge Date: December 202...  \n"
     ]
    }
   ],
   "source": [
    "df = load_enhanced_dataframe()\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UV Environment",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
