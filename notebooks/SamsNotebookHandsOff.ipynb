{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe864b11",
   "metadata": {},
   "source": [
    "# Learned probe direction projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39521256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report\n",
    "from probe_gen.config import ConfigDict\n",
    "import torch\n",
    "\n",
    "# Load the best hyperparameters or set your own\n",
    "dataset_name_1 = \"lists_llama_3b_5k\"\n",
    "dataset_name_2 = \"lists_shakespeare_llama_3b_4k\"\n",
    "\n",
    "#cfg = ConfigDict.from_json(probe_type, f\"{dataset_name_1}+{dataset_name_2}\")\n",
    "cfg = ConfigDict(layer=12, use_bias=True, normalize=True, C=0.01)\n",
    "\n",
    "# Create train, val, and test datasets\n",
    "activations_tensor_1, attention_mask_1, labels_tensor_1 = probes.load_hf_activations_and_labels_at_layer(dataset_name_1, layer=cfg.layer, verbose=True)\n",
    "activations_tensor_1 = probes.MeanAggregation()(activations_tensor_1, attention_mask_1)\n",
    "train_dataset_1, val_dataset_1, _ = probes.create_activation_datasets(activations_tensor_1, labels_tensor_1, splits=[3500, 500, 0], verbose=True)\n",
    "\n",
    "activations_tensor_2, attention_mask_2, labels_tensor_2 = probes.load_hf_activations_and_labels_at_layer(dataset_name_2, layer=cfg.layer, verbose=True)\n",
    "activations_tensor_2 = probes.MeanAggregation()(activations_tensor_2, attention_mask_2)\n",
    "train_dataset_2, val_dataset_2, _ = probes.create_activation_datasets(activations_tensor_2, labels_tensor_2, splits=[3500, 500, 0], verbose=True)\n",
    "\n",
    "probe_1 = probes.SklearnLogisticProbe(cfg)\n",
    "probe_1.fit(train_dataset_1, val_dataset_1)\n",
    "\n",
    "probe_2 = probes.SklearnLogisticProbe(cfg)\n",
    "probe_2.fit(train_dataset_2, val_dataset_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12113d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_tensor = torch.cat([activations_tensor_1, activations_tensor_2], dim=0)\n",
    "labels_tensor = torch.cat([labels_tensor_1, labels_tensor_2], dim=0)\n",
    "train_dataset_mix, val_dataset_mix, _ = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[7000, 1000, 0], verbose=True)\n",
    "\n",
    "probe_mix = probes.SklearnLogisticProbe(cfg)\n",
    "probe_mix.fit(train_dataset_mix, val_dataset_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad48ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def convert_normalized_probe_to_raw(w_norm, b_norm, mean, std):\n",
    "    \"\"\"\n",
    "    Convert probe weights learned on normalized data to equivalent weights for raw data.\n",
    "    \n",
    "    Args:\n",
    "        w_norm: weights learned on normalized activations (shape [D])\n",
    "        b_norm: bias learned on normalized activations (scalar)\n",
    "        mean: normalization mean vector (shape [D])\n",
    "        std: normalization std vector (shape [D])\n",
    "    \n",
    "    Returns:\n",
    "        w_raw: equivalent weights for raw activations (shape [D])\n",
    "        b_raw: equivalent bias for raw activations (scalar)\n",
    "    \"\"\"\n",
    "    w_raw = w_norm / std\n",
    "    b_raw = b_norm - torch.dot(w_norm / std, mean)\n",
    "    \n",
    "    return w_raw, b_raw\n",
    "\n",
    "# Usage:\n",
    "w_norm_1 = torch.tensor(probe_1.classifier.coef_.flatten()).to(torch.float32)\n",
    "b_norm_1 = probe_1.classifier.intercept_[0]\n",
    "mean_1 = torch.tensor(probe_1.transformation_mean.flatten()).to(torch.float32)\n",
    "std_1 = torch.tensor(probe_1.transformation_std.flatten()).to(torch.float32)\n",
    "\n",
    "w_raw_1, b_raw_1 = convert_normalized_probe_to_raw(w_norm_1, b_norm_1, mean_1, std_1)\n",
    "\n",
    "# Same for probe 2\n",
    "w_norm_2 = torch.tensor(probe_2.classifier.coef_.flatten()).to(torch.float32)\n",
    "b_norm_2 = probe_2.classifier.intercept_[0]\n",
    "mean_2 = torch.tensor(probe_2.transformation_mean.flatten()).to(torch.float32)\n",
    "std_2 = torch.tensor(probe_2.transformation_std.flatten()).to(torch.float32)\n",
    "\n",
    "w_raw_2, b_raw_2 = convert_normalized_probe_to_raw(w_norm_2, b_norm_2, mean_2, std_2)\n",
    "\n",
    "# Same for probe mix\n",
    "w_norm_mix = torch.tensor(probe_mix.classifier.coef_.flatten()).to(torch.float32)\n",
    "b_norm_mix = probe_mix.classifier.intercept_[0]\n",
    "mean_mix = torch.tensor(probe_mix.transformation_mean.flatten()).to(torch.float32)\n",
    "std_mix = torch.tensor(probe_mix.transformation_std.flatten()).to(torch.float32)\n",
    "\n",
    "w_raw_mix, b_raw_mix = convert_normalized_probe_to_raw(w_norm_mix, b_norm_mix, mean_mix, std_mix)\n",
    "\n",
    "print(w_raw_mix.shape)\n",
    "a = torch.dot(F.normalize(w_raw_mix, dim=0), F.normalize(w_raw_1, dim=0))\n",
    "b = torch.dot(F.normalize(w_raw_mix, dim=0), F.normalize(w_raw_2, dim=0))\n",
    "\n",
    "# Rest of your plotting code...\n",
    "positive_activations_1 = activations_tensor_1[labels_tensor_1 == 1]\n",
    "negative_activations_1 = activations_tensor_1[labels_tensor_1 == 0]\n",
    "positive_activations_2 = activations_tensor_2[labels_tensor_2 == 1]\n",
    "negative_activations_2 = activations_tensor_2[labels_tensor_2 == 0]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot scatter points\n",
    "projections_x = positive_activations_1 @ w_raw_1\n",
    "projections_y = positive_activations_1 @ w_raw_2\n",
    "plt.scatter(projections_x, projections_y, label=\"Multiple choice - Pos\", color='red', alpha=0.1)\n",
    "\n",
    "projections_x = negative_activations_1 @ w_raw_1\n",
    "projections_y = negative_activations_1 @ w_raw_2\n",
    "plt.scatter(projections_x, projections_y, label=\"Multiple choice - Neg\", color='blue', alpha=0.1)\n",
    "\n",
    "projections_x = positive_activations_2 @ w_raw_1\n",
    "projections_y = positive_activations_2 @ w_raw_2\n",
    "plt.scatter(projections_x, projections_y, label=\"Arguments - Pos\", color='orange', alpha=0.1)\n",
    "\n",
    "projections_x = negative_activations_2 @ w_raw_1\n",
    "projections_y = negative_activations_2 @ w_raw_2\n",
    "plt.scatter(projections_x, projections_y, label=\"Arguments - Neg\", color='green', alpha=0.1)\n",
    "\n",
    "# Plot corrected decision boundaries\n",
    "plt.axvline(x=-b_raw_1, color='red', linestyle='--', linewidth=2, \n",
    "            label='Multiple Choice Probe Decision Boundary')\n",
    "plt.axhline(y=-b_raw_2, color='blue', linestyle='--', linewidth=2, \n",
    "            label='Arguments Probe Decision Boundary')\n",
    "\n",
    "\n",
    "\n",
    "def get_decision_line_points(A, B, C, xlim, ylim):\n",
    "    \"\"\"Find intersection points of line A*x + B*y + C = 0 with plot boundaries\"\"\"\n",
    "    points = []\n",
    "    \n",
    "    # Check intersections with x boundaries\n",
    "    for x_bound in xlim:\n",
    "        if abs(B) > 1e-10:\n",
    "            y = -(A * x_bound + C) / B\n",
    "            if ylim[0] <= y <= ylim[1]:\n",
    "                points.append((x_bound, y))\n",
    "    \n",
    "    # Check intersections with y boundaries  \n",
    "    for y_bound in ylim:\n",
    "        if abs(A) > 1e-10:\n",
    "            x = -(B * y_bound + C) / A\n",
    "            if xlim[0] <= x <= xlim[1]:\n",
    "                points.append((x, y_bound))\n",
    "    \n",
    "    return points[:2]  # We only need 2 points to draw a line\n",
    "\n",
    "A = torch.dot(w_raw_mix, direction_1_normalized)  # coefficient of α (x-axis)\n",
    "B = torch.dot(w_raw_mix, direction_2_normalized)  # coefficient of β (y-axis)\n",
    "C = b_raw_mix\n",
    "\n",
    "# Get current plot limits to draw line across entire plot\n",
    "xlim = plt.xlim()\n",
    "ylim = plt.ylim()\n",
    "\n",
    "points = get_decision_line_points(A, B, C, xlim, ylim)\n",
    "\n",
    "plt.plot([points[0][0], points[1][0]], [points[0][1], points[1][1]], color='green', linestyle='--', linewidth=2,\n",
    "                label='Both Probe Decision Boundary')\n",
    "\n",
    "\n",
    "plt.xlabel('Projection onto Multiple Choice Probe')\n",
    "plt.ylabel('Projection onto Arguments Probe')\n",
    "legend = plt.legend()\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_alpha(1.0)\n",
    "plt.title('Probe Directions and Decision Boundaries (Sycophancy)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45bcf7d",
   "metadata": {},
   "source": [
    "# Does use_bias change anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report\n",
    "from probe_gen.config import ConfigDict\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the best hyperparameters or set your own\n",
    "dataset_names = [\n",
    "    \"sycophancy_ministral_8b_4k\",\n",
    "]\n",
    "cosine_similarities = []\n",
    "\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "\n",
    "    # Create train, val, and test datasets\n",
    "    activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(dataset_name, 12, verbose=True)\n",
    "    activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "    train_dataset, _, _ = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[3500, 0, 0], verbose=True)\n",
    "\n",
    "    cfg_1 = ConfigDict(layer=12, use_bias=True, normalize=True, C=0.01)\n",
    "    probe_1 = probes.SklearnLogisticProbe(cfg_1)\n",
    "    probe_1.fit(train_dataset, None)\n",
    "\n",
    "    cfg_2 = ConfigDict(layer=12, use_bias=False, normalize=True, C=0.01)\n",
    "    probe_2 = probes.SklearnLogisticProbe(cfg_2)\n",
    "    probe_2.fit(train_dataset, None)\n",
    "\n",
    "    weights_1 = probe_1.classifier.coef_.flatten()\n",
    "    weights_2 = probe_2.classifier.coef_.flatten() \n",
    "\n",
    "    sim = cosine_similarity([weights_1], [weights_2])[0, 0]\n",
    "    print(f\"{dataset_name}: {sim}\")\n",
    "    cosine_similarities.append(sim.item())\n",
    "\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3475ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9921962411168163, 0.9909800209336572, 0.9938872232115757, 0.9826005679340069, 0.9888519132159277, 0.9893070271046311]\n"
     ]
    }
   ],
   "source": [
    "array = []\n",
    "for x in cosine_similarities:\n",
    "    array.append(x.item())\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a57b07",
   "metadata": {},
   "source": [
    "[0.9921962411168163, 0.9909800209336572, 0.9938872232115757, 0.9826005679340069, 0.9888519132159277, 0.9893070271046311, 0.9956215483248319, 0.9990389554915939]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef1534",
   "metadata": {},
   "source": [
    "# Dataset size test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07153d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import probe_gen.probes as probes\n",
    "\n",
    "train_dataset_names = ['science_llama_3b_5k', 'science_llama_3b_prompted_5k', 'science_qwen_3b_5k']\n",
    "test_dataset_name = 'science_llama_3b_1k'\n",
    "\n",
    "# Get train activations and labels\n",
    "train_activations_tensors = []\n",
    "train_labels_tensors = []\n",
    "for i in range(len(train_dataset_names)):\n",
    "    activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(train_dataset_names[i], 12, verbose=True)\n",
    "    activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "    train_activations_tensors.append(activations_tensor)\n",
    "    train_labels_tensors.append(labels_tensor)\n",
    "\n",
    "# Create test dataset\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer(test_dataset_name, 12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "_, _, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[0, 0, 1000], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f303f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.config import ConfigDict\n",
    "\n",
    "train_sizes = [i for i in range(500, 5000, 200)]\n",
    "\n",
    "rocs = [[] for _ in range(len(train_dataset_names))]\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    print(f\"##### Training with size {train_size} #####\")\n",
    "    # Create train, val, and test datasets\n",
    "    for i in range(len(train_dataset_names)):\n",
    "        train_dataset, _, _ = probes.create_activation_datasets(train_activations_tensors[i], train_labels_tensors[i], splits=[train_size, 0, 0], verbose=False)\n",
    "        probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=True, C=0.01, normalize=True))\n",
    "        probe.fit(train_dataset, None)\n",
    "        eval_dict, _, _ = probe.eval(test_dataset)\n",
    "        rocs[i].append(eval_dict['roc_auc'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52623baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each line with different styles and labels\n",
    "plt.plot(train_sizes, rocs[0], label='On', marker='o', linewidth=2)\n",
    "plt.plot(train_sizes, rocs[1], label='Prompted', marker='s', linewidth=2)\n",
    "plt.plot(train_sizes, rocs[2], label='Off', marker='^', linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Effect of train size on generalization to on policy (science)')\n",
    "plt.xlabel('Train size')\n",
    "plt.ylabel('roc_auc')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Optional: set y-axis limits if needed\n",
    "# plt.ylim(70, 100)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd2e52",
   "metadata": {},
   "source": [
    "# Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from probe_gen.config import ConfigDict\n",
    "\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer('sycophancy_llama_3b_4k', 12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "# Create train, val, and test datasets\n",
    "train_dataset, val_dataset, _ = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[3500, 500, 0], verbose=True)\n",
    "\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer('sycophancy_arguments_llama_3b_4k', 12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "# Create train, val, and test datasets\n",
    "_, _, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[0, 0, 1000], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=True, C=0.01, normalize=True))\n",
    "probe.fit(train_dataset, val_dataset)\n",
    "eval_dict, y_pred, y_pred_proba = probe.eval(test_dataset)\n",
    "\n",
    "cm = confusion_matrix(test_dataset['y'], y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=\"Reds\")\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Set tick marks\n",
    "class_names = np.arange(cm.shape[0])\n",
    "ax.set(\n",
    "    xticks=np.arange(cm.shape[1]),\n",
    "    yticks=np.arange(cm.shape[0]),\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    ylabel=\"True label\",\n",
    "    xlabel=\"Predicted label\"\n",
    ")\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add text labels\n",
    "fmt = \"d\"\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc95e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "297bdade",
   "metadata": {},
   "source": [
    "# Significance of small roc_aucs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b318a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import probe_gen.probes as probes\n",
    "from sklearn.metrics import classification_report\n",
    "from probe_gen.config import ConfigDict\n",
    "\n",
    "\n",
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer('sycophancy_llama_3b_4k', 12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "# Create train, val, and test datasets\n",
    "train_dataset, val_dataset, _ = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[3500, 500, 0], verbose=True)\n",
    "\n",
    "probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=True, C=0.01, normalize=True))\n",
    "probe.fit(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b668f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_tensor, attention_mask, labels_tensor = probes.load_hf_activations_and_labels_at_layer('sycophancy_arguments_llama_3b_4k', 12, verbose=True)\n",
    "activations_tensor = probes.MeanAggregation()(activations_tensor, attention_mask)\n",
    "# Create train, val, and test datasets\n",
    "_, _, test_dataset = probes.create_activation_datasets(activations_tensor, labels_tensor, splits=[0, 0, 1000], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_unit_vector(n: int) -> np.ndarray:\n",
    "    v = np.random.normal(size=(1, n))   # Gaussian sample\n",
    "    v /= np.linalg.norm(v)              # Normalize to unit length\n",
    "    return v\n",
    "\n",
    "rand_probe = probes.SklearnLogisticProbe(ConfigDict(use_bias=True, C=0.01, normalize=True))\n",
    "rand_probe.fit(train_dataset, val_dataset)\n",
    "\n",
    "roc_aucs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    # Set random probe direction\n",
    "    rand_probe.classifier.coef_ = random_unit_vector(probe.classifier.coef_.shape[1])\n",
    "\n",
    "    # Check if cosine similarity is near orthogonal\n",
    "    weights_1 = probe.classifier.coef_.flatten()\n",
    "    weights_2 = rand_probe.classifier.coef_.flatten()\n",
    "    if abs(cosine_similarity([weights_1], [weights_2])[0, 0]) < 0.018:\n",
    "        eval_roc_auc = rand_probe.eval(test_dataset)[0]['roc_auc']\n",
    "        roc_aucs.append(eval_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(roc_aucs, bins=25, edgecolor='black')  # adjust bins as needed\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of roc_aucs for near orthogonal vectors\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
